{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quangminh18112003/DoAnC-/blob/main/XLNNTN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ƒê·ªí √ÅN X·ª¨ L√ù NG√îN NG·ªÆ T·ª∞ NHI√äN\n",
        "## D·ªäCH M√ÅY ANH-PH√ÅP V·ªöI M√î H√åNH ENCODER-DECODER LSTM\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jwaoqdrNeRAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C√¢u 1: X·ª≠ l√Ω d·ªØ li·ªáu (Tokenization, Vocabulary, Padding, DataLoader)"
      ],
      "metadata": {
        "id": "Cu_yiF7vHMqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### 1.1. Chu·∫©n b·ªã M√¥i tr∆∞·ªùng & D·ªØ li·ªáu\n",
        "\n",
        "**M·ª•c ti√™u:**  \n",
        "- Clone dataset Multi30K t·ª´ GitHub\n",
        "- C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán: SpaCy, NLTK, Matplotlib, PyTorch\n",
        "- Download m√¥ h√¨nh ng√¥n ng·ªØ cho ti·∫øng Anh v√† ti·∫øng Ph√°p"
      ],
      "metadata": {
        "id": "w1FyifyuD13U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clone dataset (L·∫•y d·ªØ li·ªáu v·ªÅ)\n",
        "!git clone https://github.com/multi30k/dataset.git\n",
        "\n",
        "# 2. C√†i ƒë·∫∑t th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "!pip install -U spacy nltk matplotlib\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm\n",
        "\n",
        "# 3. Download NLTK data cho BLEU score\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "5hCuwHPo-0Bl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139350e6-d23d-492f-9b70-76785365f769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'dataset' already exists and is not an empty directory.\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.8)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting fr-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
            "Successfully installed fr-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.2. Load v√† Tokenization d·ªØ li·ªáu\n",
        "\n",
        "**C√°c b∆∞·ªõc:**\n",
        "- ƒê·ªçc file `.gz` (train, val, test)\n",
        "- Tokenization v·ªõi SpaCy (`en_core_web_sm` cho ti·∫øng Anh, `fr_core_news_sm` cho ti·∫øng Ph√°p)\n",
        "- ƒê·∫£o ng∆∞·ª£c chu·ªói ti·∫øng Ph√°p (c·∫£i thi·ªán LSTM theo Sutskever et al. 2014)\n",
        "- Chu·∫©n b·ªã cho b∆∞·ªõc x√¢y d·ª±ng Vocabulary"
      ],
      "metadata": {
        "id": "SXUKleKERkd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Import th∆∞ vi·ªán c·∫ßn thi·∫øt cho vi·ªác x·ª≠ l√Ω d·ªØ li·ªáu\n",
        "print(\" ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\")"
      ],
      "metadata": {
        "id": "xNI9x7dZRmaw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9903860c-c929-4ba9-9e7b-b6242c8cf0fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ƒê√£ import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3. X√¢y d·ª±ng T·ª´ ƒëi·ªÉn (Vocabulary)\n",
        "\n",
        "**M√¥ t·∫£:**  \n",
        "Chuy·ªÉn t·ª´ v·ª±ng sang s·ªë (Index) ƒë·ªÉ ƒë∆∞a v√†o Neural Network.\n",
        "\n",
        "**Special Tokens:**\n",
        "- `<PAD>`: Padding\n",
        "- `<SOS>`: Start of Sentence\n",
        "- `<EOS>`: End of Sentence\n",
        "- `<UNK>`: Unknown word"
      ],
      "metadata": {
        "id": "YMNBOoH1RxVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import gzip\n",
        "import os\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# 1. ƒê∆∞·ªùng d·∫´n ƒë·∫øn d·ªØ li·ªáu\n",
        "data_dir = 'dataset/data/task1/raw'\n",
        "\n",
        "# 2. Load m√¥ h√¨nh ng√¥n ng·ªØ ƒë·ªÉ t√°ch t·ª´ (Tokenization)\n",
        "try:\n",
        "    spacy_fr = spacy.load('fr_core_news_sm')\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "except OSError:\n",
        "    print(\"ƒêang t·∫£i th∆∞ vi·ªán ng√¥n ng·ªØ Spacy...\")\n",
        "    os.system('python -m spacy download en_core_web_sm')\n",
        "    os.system('python -m spacy download fr_core_news_sm')\n",
        "    spacy_fr = spacy.load('fr_core_news_sm')\n",
        "    spacy_en = spacy.load('en_core_web_sm')\n",
        "\n",
        "def tokenize_fr(text):\n",
        "    \"\"\"H√†m t√°ch t·ª´ ti·∫øng Ph√°p & ƒê·∫£o ng∆∞·ª£c chu·ªói (c·∫£i thi·ªán hi·ªáu nƒÉng LSTM)\"\"\"\n",
        "    return [tok.text for tok in spacy_fr.tokenizer(text)][::-1]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"H√†m t√°ch t·ª´ ti·∫øng Anh\"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
        "\n",
        "# 3. H√†m ƒë·ªçc file .gz\n",
        "def load_data(filepath):\n",
        "    if not os.path.exists(filepath):\n",
        "        print(f\"C·∫¢NH B√ÅO: Kh√¥ng t√¨m th·∫•y file {filepath}\")\n",
        "        return []\n",
        "    with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
        "        return [line.strip() for line in f]\n",
        "\n",
        "# 4. Load d·ªØ li·ªáu (Train, Val, Test)\n",
        "print(\"ƒêang t·∫£i d·ªØ li·ªáu...\")\n",
        "\n",
        "# Train\n",
        "train_src = load_data(os.path.join(data_dir, 'train.en.gz'))\n",
        "train_trg = load_data(os.path.join(data_dir, 'train.fr.gz'))\n",
        "\n",
        "# Validation\n",
        "val_src = load_data(os.path.join(data_dir, 'val.en.gz'))\n",
        "val_trg = load_data(os.path.join(data_dir, 'val.fr.gz'))\n",
        "\n",
        "# Test 2016 Flickr\n",
        "test_src = load_data(os.path.join(data_dir, 'test_2016_flickr.en.gz'))\n",
        "test_trg = load_data(os.path.join(data_dir, 'test_2016_flickr.fr.gz'))\n",
        "\n",
        "# 5. Ki·ªÉm tra th√¥ng tin d·ªØ li·ªáu\n",
        "print(f\"------------ TH·ªêNG K√ä D·ªÆ LI·ªÜU ------------\")\n",
        "print(f\"S·ªë l∆∞·ª£ng c√¢u Train     : {len(train_src)}\")\n",
        "print(f\"S·ªë l∆∞·ª£ng c√¢u Val       : {len(val_src)}\")\n",
        "print(f\"S·ªë l∆∞·ª£ng c√¢u Test 2016 : {len(test_src)}\")\n",
        "print(f\"------------------------------------------\")\n",
        "\n",
        "if len(train_src) > 0:\n",
        "    print(f\"V√≠ d·ª• Anh (Train): {train_src[0]}\")\n",
        "    print(f\"V√≠ d·ª• Ph√°p (Train): {train_trg[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIoD-u7cR0eI",
        "outputId": "fb259cd6-26f4-40e0-a4cc-e552f9f3cd02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang t·∫£i d·ªØ li·ªáu...\n",
            "------------ TH·ªêNG K√ä D·ªÆ LI·ªÜU ------------\n",
            "S·ªë l∆∞·ª£ng c√¢u Train     : 29000\n",
            "S·ªë l∆∞·ª£ng c√¢u Val       : 1014\n",
            "S·ªë l∆∞·ª£ng c√¢u Test 2016 : 1000\n",
            "------------------------------------------\n",
            "V√≠ d·ª• Anh (Train): Two young, White males are outside near many bushes.\n",
            "V√≠ d·ª• Ph√°p (Train): Deux jeunes hommes blancs sont dehors pr√®s de buissons.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4. Dataset & DataLoader v·ªõi Padding v√† Packing\n",
        "\n",
        "**Y√äU C·∫¶U ƒê·ªÄ B√ÄI:**\n",
        "- S·ª≠ d·ª•ng `pack_padded_sequence` ƒë·ªÉ x·ª≠ l√Ω sequences c√≥ ƒë·ªô d√†i kh√°c nhau\n",
        "- S·∫Øp x·∫øp batch theo ƒë·ªô d√†i gi·∫£m d·∫ßn (`enforce_sorted=True`)\n",
        "- Batch size: 32-128\n",
        "\n",
        "**L·ª£i √≠ch c·ªßa pack_padded_sequence:**\n",
        "- Gi·∫£m t√≠nh to√°n kh√¥ng c·∫ßn thi·∫øt tr√™n padding tokens\n",
        "- TƒÉng t·ªëc ƒë·ªô training\n",
        "- C·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh"
      ],
      "metadata": {
        "id": "8WkcABMXR8J8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold=2, max_size=10000):\n",
        "        # Kh·ªüi t·∫°o 4 token ƒë·∫∑c bi·ªát\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.max_size = max_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def build_vocabulary(self, sentence_list, tokenizer):\n",
        "        # 1. ƒê·∫øm t·∫ßn su·∫•t to√†n b·ªô c√°c t·ª´ trong dataset\n",
        "        frequencies = Counter()\n",
        "        for sentence in sentence_list:\n",
        "            for word in tokenizer(sentence):\n",
        "                frequencies[word] += 1\n",
        "\n",
        "        # 2. L·∫•y danh s√°ch c√°c t·ª´ ph·ªï bi·∫øn nh·∫•t theo max_size\n",
        "        # Tr·ª´ ƒëi 4 v√¨ ƒë√£ c√≥ 4 token ƒë·∫∑c bi·ªát (<PAD>, <SOS>, <EOS>, <UNK>)\n",
        "        most_common_words = frequencies.most_common(self.max_size - 4)\n",
        "\n",
        "        # 3. Th√™m v√†o t·ª´ ƒëi·ªÉn n·∫øu ƒë·∫°t ng∆∞·ª°ng freq_threshold\n",
        "        idx = 4\n",
        "        for word, count in most_common_words:\n",
        "            if count >= self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "    def numericalize(self, text, tokenizer):\n",
        "        tokenized_text = tokenizer(text)\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]\n",
        "\n",
        "# --- S·ª¨ D·ª§NG ---\n",
        "print(\"ƒêang x√¢y d·ª±ng t·ª´ ƒëi·ªÉn (Gi·ªõi h·∫°n 10,000 t·ª´)...\")\n",
        "\n",
        "# üîß CH·ªêNG OVERFITTING: freq_threshold=2 lo·∫°i b·ªè t·ª´ xu·∫•t hi·ªán ch·ªâ 1 l·∫ßn\n",
        "vocab_en = Vocabulary(freq_threshold=2, max_size=10000)\n",
        "vocab_en.build_vocabulary(train_src, tokenize_en)\n",
        "\n",
        "vocab_fr = Vocabulary(freq_threshold=2, max_size=10000)\n",
        "vocab_fr.build_vocabulary(train_trg, tokenize_fr)\n",
        "\n",
        "print(f\"‚úÖ K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn EN: {len(vocab_en):,} t·ª´\")\n",
        "print(f\"‚úÖ K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn FR: {len(vocab_fr):,} t·ª´\")\n",
        "print(f\"üîß freq_threshold=2 (lo·∫°i b·ªè t·ª´ xu·∫•t hi·ªán ch·ªâ 1 l·∫ßn)\")\n",
        "# L∆∞u √Ω: N·∫øu dataset nh·ªè h∆°n 10,000 t·ª´ v·ª±ng th√¨ n√≥ s·∫Ω l·∫•y h·∫øt s·ªë t·ª´ hi·ªán c√≥."
      ],
      "metadata": {
        "id": "YQPe7BrvR89M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49fde25c-7b05-4cc9-fba3-2c55b2e0b17e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ƒêang x√¢y d·ª±ng t·ª´ ƒëi·ªÉn (Gi·ªõi h·∫°n 10,000 t·ª´)...\n",
            "‚úÖ K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn EN: 6,191 t·ª´\n",
            "‚úÖ K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn FR: 6,555 t·ª´\n",
            "üîß freq_threshold=2 (lo·∫°i b·ªè t·ª´ xu·∫•t hi·ªán ch·ªâ 1 l·∫ßn)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## C√¢u 2: Tri·ªÉn khai m√¥ h√¨nh Encoder-Decoder LSTM v·ªõi context vector c√≥ ƒë·ªãnh h∆∞·ªõng\n",
        "\n"
      ],
      "metadata": {
        "id": "CCZlC9xPR-cG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1. X√¢y d·ª±ng Encoder-Decoder LSTM v·ªõi Context Vector\n",
        "\n",
        "**Ki·∫øn tr√∫c Seq2Seq (theo y√™u c·∫ßu ƒë·ªÅ b√†i):**\n",
        "\n",
        "**Encoder:**\n",
        "- Input: Chu·ªói token ti·∫øng Anh ‚Üí embedding (size 256)\n",
        "- LSTM (2 layers, hidden=512, dropout=0.5)\n",
        "- S·ª≠ d·ª•ng `pack_padded_sequence` ƒë·ªÉ b·ªè qua padding\n",
        "- Output: Context Vector = (hidden state, cell state) cu·ªëi c√πng\n",
        "\n",
        "**Decoder:**\n",
        "- Input: Context Vector t·ª´ Encoder + Target sequence\n",
        "- LSTM (2 layers, hidden=512, dropout=0.5)\n",
        "- Linear layer ‚Üí Vocabulary size\n",
        "- Teacher Forcing v·ªõi Scheduled Sampling (0.5)"
      ],
      "metadata": {
        "id": "BZ7UIG3lD_k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_lengths):\n",
        "        # src = [src len, batch size]\n",
        "        # src_lengths = [batch size] - ƒë·ªô d√†i th·ª±c c·ªßa m·ªói c√¢u\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        # Pack padded sequence ƒë·ªÉ LSTM b·ªè qua padding (y√™u c·∫ßu ƒë·ªÅ b√†i)\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n",
        "            embedded, src_lengths.cpu(), enforce_sorted=True\n",
        "        )\n",
        "\n",
        "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)\n",
        "\n",
        "        # Unpack sequence (kh√¥ng d√πng outputs n√™n c√≥ th·ªÉ b·ªè qua)\n",
        "        # outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "\n",
        "        return hidden, cell\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.output_dim = output_dim\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, src, trg, src_lengths, teacher_forcing_ratio=0.5):\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        hidden, cell = self.encoder(src, src_lengths)\n",
        "\n",
        "        input = trg[0,:] # Token <SOS>\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n",
        "\n",
        "print(\" ƒê√£ ƒë·ªãnh nghƒ©a ki·∫øn tr√∫c Encoder-Decoder LSTM\")"
      ],
      "metadata": {
        "id": "cY7y13UzSBL5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b75de69-fc2a-4743-f99e-b63f8f0bcb8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ƒê√£ ƒë·ªãnh nghƒ©a ki·∫øn tr√∫c Encoder-Decoder LSTM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2. T·∫°o DataLoader & Kh·ªüi t·∫°o m√¥ h√¨nh\n",
        "\n",
        "**Dataset & DataLoader:**\n",
        "- S·ª≠ d·ª•ng class `Multi30kDataset` t√πy ch·ªânh\n",
        "- Padding sequences v·ªõi `pad_sequence`\n",
        "- S·∫Øp x·∫øp theo ƒë·ªô d√†i gi·∫£m d·∫ßn (cho `pack_padded_sequence`)\n",
        "- Batch size: 32 (gi·∫£m ƒë·ªÉ ch·ªëng overfitting)\n",
        "\n",
        "**Hyperparameters (theo ƒë·ªÅ b√†i):**\n",
        "- Embedding dimension: 256\n",
        "- Hidden dimension: 512\n",
        "- Number of layers: 2\n",
        "- Dropout: 0.5 (m·ª©c cao nh·∫•t ƒë·ªÉ ch·ªëng overfitting)\n",
        "- Device: GPU n·∫øu c√≥, CPU n·∫øu kh√¥ng"
      ],
      "metadata": {
        "id": "Dt7t3YhOSD3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Multi30kDataset(Dataset):\n",
        "    def __init__(self, src_lines, trg_lines, vocab_src, vocab_trg):\n",
        "        self.src_lines = src_lines\n",
        "        self.trg_lines = trg_lines\n",
        "        self.vocab_src = vocab_src\n",
        "        self.vocab_trg = vocab_trg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.src_lines)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        src_text = self.src_lines[index]\n",
        "        trg_text = self.trg_lines[index]\n",
        "\n",
        "        src_indices = [1] + self.vocab_src.numericalize(src_text, tokenize_en) + [2] # Th√™m SOS, EOS\n",
        "        trg_indices = [1] + self.vocab_trg.numericalize(trg_text, tokenize_fr) + [2]\n",
        "\n",
        "        return torch.tensor(src_indices), torch.tensor(trg_indices)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"H√†m n√†y ƒë·ªÉ padding c√°c c√¢u cho b·∫±ng ƒë·ªô d√†i nhau trong 1 batch\"\"\"\n",
        "    src_batch, trg_batch = [], []\n",
        "    src_lengths = []\n",
        "\n",
        "    for src_item, trg_item in batch:\n",
        "        src_batch.append(src_item)\n",
        "        trg_batch.append(trg_item)\n",
        "        src_lengths.append(len(src_item))\n",
        "\n",
        "    # S·∫Øp x·∫øp theo ƒë·ªô d√†i gi·∫£m d·∫ßn (y√™u c·∫ßu c·ªßa pack_padded_sequence)\n",
        "    src_lengths = torch.tensor(src_lengths)\n",
        "    sorted_indices = src_lengths.argsort(descending=True)\n",
        "\n",
        "    src_batch = [src_batch[i] for i in sorted_indices]\n",
        "    trg_batch = [trg_batch[i] for i in sorted_indices]\n",
        "    src_lengths = src_lengths[sorted_indices]\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=0) # 0 l√† <PAD>\n",
        "    trg_batch = pad_sequence(trg_batch, padding_value=0)\n",
        "\n",
        "    return src_batch, trg_batch, src_lengths\n",
        "\n",
        "# üîß CH·ªêNG OVERFITTING: TƒÉng batch size ƒë·ªÉ gi·∫£m noise\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "# T·∫°o DataLoader cho Train\n",
        "train_dataset = Multi30kDataset(train_src, train_trg, vocab_en, vocab_fr)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# T·∫°o DataLoader cho Validation\n",
        "val_dataset = Multi30kDataset(val_src, val_trg, vocab_en, vocab_fr)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# T·∫°o DataLoader cho Test\n",
        "test_dataset = Multi30kDataset(test_src, test_trg, vocab_en, vocab_fr)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"‚úÖ ƒê√£ t·∫°o DataLoader v·ªõi batch_size={BATCH_SIZE}\")\n",
        "\n",
        "# ============================================================================\n",
        "# KH·ªûI T·∫†O M√î H√åNH - GI·∫¢M MODEL COMPLEXITY ƒê·ªÇ CH·ªêNG OVERFITTING\n",
        "# ============================================================================\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "INPUT_DIM = len(vocab_en)\n",
        "OUTPUT_DIM = len(vocab_fr)\n",
        "\n",
        "# üîß GI·∫¢M MODEL SIZE ƒë·ªÉ ch·ªëng overfitting\n",
        "ENC_EMB_DIM = 512      # Gi·ªØ 256\n",
        "DEC_EMB_DIM = 512      # Gi·ªØ 256\n",
        "HID_DIM = 512          # üîß GI·∫¢M t·ª´ 512 ‚Üí 256 (gi·∫£m complexity)\n",
        "N_LAYERS = 2           # Gi·ªØ 2 layers\n",
        "ENC_DROPOUT = 0.5      # Gi·ªØ 0.5\n",
        "DEC_DROPOUT = 0.5      # Gi·ªØ 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc, dec, device).to(device)\n",
        "\n",
        "print(f\"‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o tr√™n: {device}\")\n",
        "print(f\"üìä C·∫•u h√¨nh: EMB={ENC_EMB_DIM}, HID={HID_DIM} (GI·∫¢M), LAYERS={N_LAYERS}, DROPOUT={ENC_DROPOUT}\")\n",
        "print(f\"‚úÖ S·ªë tham s·ªë: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
      ],
      "metadata": {
        "id": "a4jq2l1bSEo6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c640ae83-efca-4c71-ad51-a487b6cb391c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ƒê√£ t·∫°o DataLoader v·ªõi batch_size=128\n",
            "‚úÖ M√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o tr√™n: cuda\n",
            "üìä C·∫•u h√¨nh: EMB=512, HID=512 (GI·∫¢M), LAYERS=2, DROPOUT=0.5\n",
            "‚úÖ S·ªë tham s·ªë: 18,293,659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C√¢u 3: Hu·∫•n luy·ªán, ƒê√°nh gi√°, Early Stopping, L∆∞u Checkpoint\n",
        "\n",
        "### 3.1. Training Loop v·ªõi Early Stopping\n",
        "\n",
        "**Thi·∫øt l·∫≠p (theo y√™u c·∫ßu ƒë·ªÅ b√†i):**\n",
        "- Optimizer: Adam (lr=0.001, weight_decay=1e-5)\n",
        "- Loss: CrossEntropyLoss (ignore padding, label_smoothing=0.1)\n",
        "- Epochs: 10-20 (max 20)\n",
        "- Early Stopping: patience = 3\n",
        "- Gradient Clipping: 1.0\n",
        "- Checkpoint: L∆∞u `best_model.pth` khi val_loss gi·∫£m\n",
        "\n",
        "**K·ªπ thu·∫≠t ch·ªëng Overfitting:**\n",
        "- Dropout 0.5\n",
        "- L2 Regularization (weight_decay)\n",
        "- Label Smoothing 0.1\n",
        "- Scheduled Sampling (TF: 1.0 ‚Üí 0.5)\n",
        "- Early Stopping"
      ],
      "metadata": {
        "id": "dwTxpS96SGRx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# üîß CH·ªêNG OVERFITTING M·∫†NH: TƒÉng weight_decay (L2 regularization)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "# üîß LABEL SMOOTHING M·∫†NH H∆†N: 0.1 ‚Üí 0.2\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0, label_smoothing=0.2)\n",
        "\n",
        "# üîß TH√äM LEARNING RATE SCHEDULER ƒë·ªÉ gi·∫£m lr khi val_loss kh√¥ng gi·∫£m\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip, epoch, total_epochs):\n",
        "    \"\"\"\n",
        "    üîß ƒê√É TH√äM SCHEDULED SAMPLING + WORD DROPOUT:\n",
        "    - Teacher forcing ratio gi·∫£m d·∫ßn t·ª´ 1.0 ‚Üí 0.5 theo epoch\n",
        "    - Word dropout: Ng·∫´u nhi√™n thay m·ªôt s·ªë t·ª´ input b·∫±ng <UNK>\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    # üîß SCHEDULED SAMPLING: Gi·∫£m d·∫ßn teacher forcing\n",
        "    teacher_forcing_ratio = max(0.5, 1.0 - 0.5 * (epoch / total_epochs))\n",
        "\n",
        "    for i, (src, trg, src_lengths) in enumerate(iterator):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # üîß WORD DROPOUT: Ng·∫´u nhi√™n thay 10% t·ª´ trong src b·∫±ng <UNK> (index 3)\n",
        "        if model.training:\n",
        "            mask = torch.rand(src.shape) < 0.1\n",
        "            mask = mask.to(device)\n",
        "            # Kh√¥ng dropout padding (index 0) v√† special tokens\n",
        "            mask = mask & (src > 3)\n",
        "            src = src.masked_fill(mask, 3)  # 3 = <UNK>\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg, src_lengths, teacher_forcing_ratio)\n",
        "\n",
        "        # Reshape ƒë·ªÉ t√≠nh loss\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator), teacher_forcing_ratio\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (src, trg, src_lengths) in enumerate(iterator):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg, src_lengths, teacher_forcing_ratio=0) # Kh√¥ng d√πng teacher forcing khi eval\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "# ============================================================================\n",
        "# HU·∫§N LUY·ªÜN V·ªöI EARLY STOPPING + LR SCHEDULER\n",
        "# ============================================================================\n",
        "CLIP = 1.0              # Gradient clipping\n",
        "N_EPOCHS = 20           # T·ªëi ƒëa 20 epochs\n",
        "PATIENCE = 3            # Early stopping patience = 3\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "epochs_no_improve = 0\n",
        "\n",
        "# L∆∞u l·ªãch s·ª≠ ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    # Train v·ªõi scheduled sampling + word dropout\n",
        "    train_loss, tf_ratio = train(model, train_loader, optimizer, criterion, CLIP, epoch, N_EPOCHS)\n",
        "\n",
        "    # Validation\n",
        "    val_loss = evaluate(model, val_loader, criterion)\n",
        "\n",
        "    # üîß Learning Rate Scheduler\n",
        "    old_lr = optimizer.param_groups[0]['lr']\n",
        "    scheduler.step(val_loss)\n",
        "    new_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    # L∆∞u l·ªãch s·ª≠\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    # T√≠nh gap\n",
        "    gap = abs(val_loss - train_loss)\n",
        "\n",
        "    # In k·∫øt qu·∫£\n",
        "    lr_info = f\" | LR: {new_lr:.6f}\" if new_lr != old_lr else \"\"\n",
        "    print(f'Epoch: {epoch+1:02}/{N_EPOCHS} | Train: {train_loss:.3f} | Val: {val_loss:.3f} | Gap: {gap:.3f} | TF: {tf_ratio:.2f}{lr_info}')\n",
        "\n",
        "    # Early Stopping Logic\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        epochs_no_improve = 0\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f'   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: {val_loss:.3f})')\n",
        "    else:\n",
        "        epochs_no_improve += 1\n",
        "        print(f'   --> ‚ö†Ô∏è Val Loss kh√¥ng c·∫£i thi·ªán ({epochs_no_improve}/{PATIENCE})')\n",
        "\n",
        "        if epochs_no_improve >= PATIENCE:\n",
        "            print(f'\\nüõë Early Stopping: Val Loss kh√¥ng gi·∫£m sau {PATIENCE} epoch.')\n",
        "            print(f'‚úÖ K·∫øt th√∫c hu·∫•n luy·ªán t·∫°i Epoch {epoch+1}')\n",
        "            break\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(f\"HO√ÄN T·∫§T! Best Validation Loss: {best_val_loss:.3f}\")\n",
        "if len(train_losses) > 0 and len(val_losses) > 0:\n",
        "    final_gap = abs(val_losses[-1] - train_losses[-1])\n",
        "    print(f\"Train/Val Gap cu·ªëi c√πng: {final_gap:.3f}\")\n",
        "    if final_gap < 0.5:\n",
        "        print(\"‚úÖ M√¥ h√¨nh t·ªïng qu√°t h√≥a R·∫§T T·ªêT (gap < 0.5)\")\n",
        "    elif final_gap < 1.0:\n",
        "        print(\"‚úÖ M√¥ h√¨nh t·ªïng qu√°t h√≥a T·ªêT (gap < 1.0)\")\n",
        "    elif final_gap < 1.5:\n",
        "        print(\"‚ö†Ô∏è Overfitting nh·∫π (1.0 < gap < 1.5)\")\n",
        "    else:\n",
        "        print(\"‚ùå Overfitting n·∫∑ng (gap >= 1.5)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load l·∫°i m√¥ h√¨nh t·ªët nh·∫•t\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "print(\"‚úÖ ƒê√£ t·∫£i l·∫°i m√¥ h√¨nh t·ªët nh·∫•t t·ª´ checkpoint 'best_model.pth'\")"
      ],
      "metadata": {
        "id": "W0MhDTGoSJ09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "656ddd26-c4bb-474e-89bc-ecf1fe3f8bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01/20 | Train: 5.669 | Val: 6.675 | Gap: 1.006 | TF: 1.00\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 6.675)\n",
            "Epoch: 02/20 | Train: 4.887 | Val: 6.582 | Gap: 1.695 | TF: 0.97\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 6.582)\n",
            "Epoch: 03/20 | Train: 4.641 | Val: 6.372 | Gap: 1.731 | TF: 0.95\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 6.372)\n",
            "Epoch: 04/20 | Train: 4.517 | Val: 6.101 | Gap: 1.584 | TF: 0.93\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 6.101)\n",
            "Epoch: 05/20 | Train: 4.426 | Val: 5.919 | Gap: 1.492 | TF: 0.90\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 5.919)\n",
            "Epoch: 06/20 | Train: 4.358 | Val: 5.854 | Gap: 1.496 | TF: 0.88\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 5.854)\n",
            "Epoch: 07/20 | Train: 4.313 | Val: 5.774 | Gap: 1.461 | TF: 0.85\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 5.774)\n",
            "Epoch: 08/20 | Train: 4.284 | Val: 5.604 | Gap: 1.320 | TF: 0.82\n",
            "   --> ‚úÖ L∆∞u checkpoint m·ªõi (Val Loss: 5.604)\n",
            "Epoch: 09/20 | Train: 4.248 | Val: 5.606 | Gap: 1.358 | TF: 0.80\n",
            "   --> ‚ö†Ô∏è Val Loss kh√¥ng c·∫£i thi·ªán (1/3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C√¢u 4: H√†m translate() ho·∫°t ƒë·ªông v·ªõi c√¢u m·ªõi\n",
        "\n",
        "**Y√™u c·∫ßu b·∫Øt bu·ªôc c·ªßa ƒë·ªÅ b√†i:**  \n",
        "Vi·∫øt h√†m `translate(sentence: str) -> str` nh·∫≠n c√¢u ti·∫øng Anh, tr·∫£ v·ªÅ c√¢u ti·∫øng Ph√°p.\n",
        "\n",
        "**Pipeline:**\n",
        "1. Tokenize c√¢u ti·∫øng Anh v·ªõi SpaCy\n",
        "2. Chuy·ªÉn tokens th√†nh indices (numericalize)\n",
        "3. Encoder ‚Üí Context Vector (hidden, cell)\n",
        "4. Decoder v·ªõi Greedy Decoding ‚Üí t·ª´ng token\n",
        "5. ƒê·∫£o ng∆∞·ª£c k·∫øt qu·∫£ (v√¨ ƒë√£ reverse khi tokenize FR)\n",
        "6. D·ª´ng khi g·∫∑p `<EOS>` ho·∫∑c ƒë·∫°t max_len=50\n",
        "7. Tr·∫£ v·ªÅ string ti·∫øng Ph√°p"
      ],
      "metadata": {
        "id": "GD08RgfZSimE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, src_tokenizer, vocab_src, vocab_trg, model, device, max_len=50):\n",
        "    \"\"\"\n",
        "    D·ªãch m·ªôt c√¢u t·ª´ ti·∫øng Anh sang ti·∫øng Ph√°p\n",
        "\n",
        "    Args:\n",
        "        sentence: C√¢u ti·∫øng Anh (string ho·∫∑c list tokens)\n",
        "        src_tokenizer: H√†m tokenize cho ti·∫øng Anh\n",
        "        vocab_src: T·ª´ ƒëi·ªÉn ti·∫øng Anh\n",
        "        vocab_trg: T·ª´ ƒëi·ªÉn ti·∫øng Ph√°p\n",
        "        model: M√¥ h√¨nh Seq2Seq\n",
        "        device: CPU ho·∫∑c GPU\n",
        "        max_len: ƒê·ªô d√†i t·ªëi ƒëa c√¢u d·ªãch (m·∫∑c ƒë·ªãnh 50 theo ƒë·ªÅ b√†i)\n",
        "\n",
        "    Returns:\n",
        "        List tokens ti·∫øng Ph√°p (ƒë√£ ƒë·∫£o ng∆∞·ª£c v·ªÅ ƒë√∫ng th·ª© t·ª±)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # X·ª≠ l√Ω c√¢u ƒë·∫ßu v√†o\n",
        "    if isinstance(sentence, str):\n",
        "        tokens = [token.text.lower() for token in src_tokenizer(sentence)]\n",
        "    else:\n",
        "        tokens = [token.lower() for token in sentence]\n",
        "\n",
        "    # Th√™m token ƒë·∫∑c bi·ªát\n",
        "    tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
        "\n",
        "    # Chuy·ªÉn th√†nh tensor\n",
        "    src_indexes = [vocab_src.stoi.get(token, vocab_src.stoi['<UNK>']) for token in tokens]\n",
        "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "    # T·∫°o src_lengths cho encoder (ƒë·ªô d√†i th·ª±c c·ªßa c√¢u)\n",
        "    src_length = torch.LongTensor([len(src_indexes)]).to(device)\n",
        "\n",
        "    # Encoder\n",
        "    with torch.no_grad():\n",
        "        hidden, cell = model.encoder(src_tensor, src_length)\n",
        "\n",
        "    # Decoder (Greedy Decoding)\n",
        "    trg_indexes = [vocab_trg.stoi['<SOS>']]\n",
        "\n",
        "    for i in range(max_len):\n",
        "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
        "\n",
        "        pred_token = output.argmax(1).item()\n",
        "        trg_indexes.append(pred_token)\n",
        "\n",
        "        # D·ª´ng khi g·∫∑p <EOS>\n",
        "        if pred_token == vocab_trg.stoi['<EOS>']:\n",
        "            break\n",
        "\n",
        "    # Chuy·ªÉn index v·ªÅ tokens\n",
        "    trg_tokens = [vocab_trg.itos[i] for i in trg_indexes]\n",
        "\n",
        "    # B·ªè <SOS> v√† <EOS>\n",
        "    trg_tokens = trg_tokens[1:]\n",
        "    if trg_tokens and trg_tokens[-1] == '<EOS>':\n",
        "        trg_tokens = trg_tokens[:-1]\n",
        "\n",
        "    # ƒê·∫¢O NG∆Ø·ª¢C v√¨ ƒë√£ reverse khi train\n",
        "    trg_tokens = trg_tokens[::-1]\n",
        "\n",
        "    return trg_tokens\n",
        "\n",
        "# ============================================================================\n",
        "# H√ÄM TRANSLATE() - Y√äU C·∫¶U B·∫ÆT BU·ªòC C·ª¶A ƒê·ªÄ B√ÄI\n",
        "# ============================================================================\n",
        "def translate(sentence: str) -> str:\n",
        "    \"\"\"\n",
        "    D·ªãch c√¢u ti·∫øng Anh sang ti·∫øng Ph√°p (Y√äU C·∫¶U ƒê·ªÄ B√ÄI)\n",
        "\n",
        "    Args:\n",
        "        sentence: C√¢u ti·∫øng Anh (string)\n",
        "\n",
        "    Returns:\n",
        "        C√¢u ti·∫øng Ph√°p ƒë√£ d·ªãch (string)\n",
        "    \"\"\"\n",
        "    tokens = translate_sentence(sentence, spacy_en, vocab_en, vocab_fr, model, device)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ============================================================================\n",
        "# DEMO: D·ªãch th·ª≠ v√†i c√¢u\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DEMO: D·ªäCH M√ÅY ANH - PH√ÅP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_sentences = [\n",
        "    \"A dog is running in the garden.\",\n",
        "    \"Two young girls are playing with toys.\",\n",
        "    \"A man is riding a bicycle.\",\n",
        "    \"The children are eating ice cream.\",\n",
        "    \"A woman is reading a book.\"\n",
        "]\n",
        "\n",
        "for sentence in test_sentences:\n",
        "    # S·ª≠ d·ª•ng h√†m translate() theo y√™u c·∫ßu ƒë·ªÅ b√†i\n",
        "    translation_text = translate(sentence)\n",
        "\n",
        "    print(f\"\\nüá¨üáß Ti·∫øng Anh: {sentence}\")\n",
        "    print(f\"üá´üá∑ Ti·∫øng Ph√°p: {translation_text}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "D_abEjveSmW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C√¢u 5: BLEU Score + Bi·ªÉu ƒë·ªì Loss + Ph√¢n t√≠ch l·ªói\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MA27yUewSoPL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1. ƒê√°nh gi√° BLEU Score\n",
        "\n",
        "**BLEU (Bilingual Evaluation Understudy):**\n",
        "- So s√°nh n-grams gi·ªØa b·∫£n d·ªãch v√† ground truth\n",
        "- S·ª≠ d·ª•ng `nltk.translate.bleu_score.sentence_bleu`\n",
        "- T√≠nh BLEU cho t·ª´ng c√¢u, sau ƒë√≥ l·∫•y trung b√¨nh\n",
        "- ƒê√°nh gi√° tr√™n test set (test_2016_flickr)\n",
        "\n",
        "**Smoothing Function:**\n",
        "- S·ª≠ d·ª•ng `SmoothingFunction().method4`\n",
        "- Tr√°nh BLEU = 0 khi thi·∫øu n-gram matches\n",
        "- Cho k·∫øt qu·∫£ ·ªïn ƒë·ªãnh h∆°n v·ªõi c√¢u ng·∫Øn\n",
        "\n",
        "**C√°ch t√≠nh:**\n",
        "1. V·ªõi m·ªói c√¢u: `sentence_bleu([reference_tokens], hypothesis_tokens, smoothing_function=smooth)`\n",
        "2. L·∫•y trung b√¨nh BLEU c·ªßa t·∫•t c·∫£ c√¢u trong test set\n",
        "\n",
        "**Test Loss:**\n",
        "- ƒê√°nh gi√° tr√™n test set\n",
        "- Ki·ªÉm tra overfitting: So s√°nh v·ªõi train/val loss"
      ],
      "metadata": {
        "id": "_A3Yi3zDDuWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# H√†m t√≠nh BLEU\n",
        "def calculate_bleu(data_src, data_trg, src_tokenizer, vocab_src, vocab_trg, model, device):\n",
        "    \"\"\"\n",
        "    T√≠nh BLEU score trung b√¨nh tr√™n t·∫≠p test b·∫±ng sentence_bleu\n",
        "    (theo y√™u c·∫ßu ƒë·ªÅ b√†i: nltk.translate.bleu_score.sentence_bleu)\n",
        "\n",
        "    S·ª≠ d·ª•ng smoothing function ƒë·ªÉ tr√°nh BLEU = 0 khi thi·∫øu n-gram\n",
        "    \"\"\"\n",
        "    bleu_scores = []\n",
        "    model.eval()\n",
        "\n",
        "    # Kh·ªüi t·∫°o smoothing function (method4 - Chen and Cherry 2014)\n",
        "    smooth = SmoothingFunction().method4\n",
        "\n",
        "    total = len(data_src)\n",
        "    print(f\"ƒêang t√≠nh to√°n BLEU score tr√™n {total} c√¢u test...\")\n",
        "    print(\"S·ª≠ d·ª•ng smoothing function (method4) ƒë·ªÉ x·ª≠ l√Ω n-gram kh√¥ng match\")\n",
        "    print(\"Vui l√≤ng ƒë·ª£i, qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 3-5 ph√∫t.\")\n",
        "\n",
        "    # T√≠nh tr√™n TO√ÄN B·ªò t·∫≠p test (theo y√™u c·∫ßu ƒë·ªÅ b√†i)\n",
        "    for idx, (src, trg) in enumerate(zip(data_src, data_trg)):\n",
        "        # In progress m·ªói 100 c√¢u\n",
        "        if (idx + 1) % 100 == 0:\n",
        "            print(f\"  ‚Üí ƒê√£ x·ª≠ l√Ω {idx + 1}/{total} c√¢u ({(idx+1)/total*100:.1f}%)\")\n",
        "\n",
        "        # QUAN TR·ªåNG: Tokenize b√¨nh th∆∞·ªùng (KH√îNG reverse) ƒë·ªÉ so s√°nh\n",
        "        # v√¨ translate_sentence ƒë√£ reverse output v·ªÅ th·ª© t·ª± ƒë√∫ng r·ªìi\n",
        "        trg_tokens = [tok.text for tok in spacy_fr.tokenizer(trg)]\n",
        "\n",
        "        pred_tokens = translate_sentence(src, src_tokenizer, vocab_src, vocab_trg, model, device)\n",
        "        if pred_tokens and pred_tokens[-1] == '<EOS>':\n",
        "            pred_tokens = pred_tokens[:-1]\n",
        "\n",
        "        # T√≠nh BLEU cho t·ª´ng c√¢u b·∫±ng sentence_bleu v·ªõi smoothing\n",
        "        # references ph·∫£i l√† list of lists (c√≥ th·ªÉ c√≥ nhi·ªÅu b·∫£n d·ªãch tham kh·∫£o)\n",
        "        bleu = sentence_bleu([trg_tokens], pred_tokens, smoothing_function=smooth)\n",
        "        bleu_scores.append(bleu)\n",
        "\n",
        "    print(f\"   Ho√†n t·∫•t {total}/{total} c√¢u (100%)\\n\")\n",
        "\n",
        "    # Tr·∫£ v·ªÅ BLEU trung b√¨nh\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "    return avg_bleu\n",
        "\n",
        "# ==============================================================================\n",
        "# TH·ª∞C THI - ƒê√°nh gi√° tr√™n Test Set\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ƒê√ÅNH GI√Å M√î H√åNH TR√äN T·∫¨P TEST\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# T√≠nh Test Loss\n",
        "test_loss = evaluate(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# T√≠nh BLEU Score\n",
        "bleu_score = calculate_bleu(test_src, test_trg, spacy_en, vocab_en, vocab_fr, model, device)\n",
        "print(f'BLEU Score: {bleu_score*100:.2f}')\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "7N3u5uGNSpFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2. V·∫Ω Bi·ªÉu ƒë·ªì Train/Val Loss\n",
        "\n",
        "**M·ª•c ƒë√≠ch:** Ph√°t hi·ªán Overfitting/Underfitting\n",
        "\n",
        "**Ph√¢n t√≠ch:**\n",
        "- Train Loss ‚Üì, Val Loss ‚Üë ‚Üí Overfitting\n",
        "- C·∫£ hai ‚Üì ‚Üí H·ªçc t·ªët\n",
        "- C·∫£ hai cao ‚Üí Underfitting"
      ],
      "metadata": {
        "id": "s44f0IssSqUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# V·∫Ω bi·ªÉu ƒë·ªì so s√°nh Train Loss vs Val Loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, len(train_losses)+1), train_losses, 'b-', label='Train Loss', marker='o')\n",
        "plt.plot(range(1, len(val_losses)+1), val_losses, 'r-', label='Validation Loss', marker='s')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Loss', fontsize=12)\n",
        "plt.title('Train Loss vs Validation Loss', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_history.png', dpi=300)\n",
        "plt.show()\n",
        "\n",
        "print(\" ƒê√£ l∆∞u bi·ªÉu ƒë·ªì v√†o file 'training_history.png'\")"
      ],
      "metadata": {
        "id": "GE7mzEHUStUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.3. Demo d·ªãch v√† ph√¢n t√≠ch l·ªói\n",
        "\n",
        "**Demo 5 c√¢u test:**\n",
        "- Hi·ªÉn th·ªã c√¢u g·ªëc (EN)\n",
        "- Hi·ªÉn th·ªã ground truth (FR)\n",
        "- Hi·ªÉn th·ªã prediction (FR)\n",
        "- So s√°nh v√† ph√¢n t√≠ch"
      ],
      "metadata": {
        "id": "nue3NGvwSwAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DEMO K·∫æT QU·∫¢ ---\n",
        "print(\"-\" * 50)\n",
        "print(\"K·∫æT QU·∫¢ D·ªäCH TH·ª¨ NGHI·ªÜM:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# L·∫•y ng·∫´u nhi√™n v√†i c√¢u trong t·∫≠p test ƒë·ªÉ d·ªãch\n",
        "indices = [0, 10, 50, 100, 150]\n",
        "\n",
        "for i in indices:\n",
        "    src = test_src[i]\n",
        "    trg = test_trg[i]\n",
        "\n",
        "    print(f'C√¢u g·ªëc (EN): {src}')\n",
        "    print(f'C√¢u m·∫´u (FR): {trg}')\n",
        "\n",
        "    # D·ªãch c√¢u\n",
        "    translation = translate_sentence(src, spacy_en, vocab_en, vocab_fr, model, device)\n",
        "    translation_text = \" \".join(translation)\n",
        "\n",
        "    print(f'M√°y d·ªãch (Pred): {translation_text}')\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "ngkEydGaSyLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4. Ph√¢n t√≠ch c√°c v·∫•n ƒë·ªÅ ƒë√£ x·ª≠ l√Ω\n",
        "\n",
        "**1. ƒê·ªô d√†i chu·ªói kh√°c nhau:**\n",
        "- Gi·∫£i ph√°p: pad_sequence + collate_fn\n",
        "\n",
        "**2. Teacher Forcing & Exposure Bias:**\n",
        "- Gi·∫£i ph√°p: teacher_forcing_ratio = 0.5\n",
        "\n",
        "**3. Overfitting:**\n",
        "- Gi·∫£i ph√°p: Dropout + Early Stopping\n",
        "\n",
        "**4. Loss kh√¥ng gi·∫£m:**\n",
        "- Gi·∫£i ph√°p: Gradient clipping + ignore padding"
      ],
      "metadata": {
        "id": "v0xeG_lCSz07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PH√ÇN T√çCH C√ÅC V·∫§N ƒê·ªÄ ƒê√É X·ª¨ L√ù\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PH√ÇN T√çCH C√ÅC V·∫§N ƒê·ªÄ ƒê√É X·ª¨ L√ù\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Ph√¢n t√≠ch ƒë·ªô d√†i c√¢u\n",
        "print(\"\\n1. PH√ÇN T√çCH ƒê·ªò D√ÄI C√ÇU:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "train_src_lengths = [len(tokenize_en(s)) for s in train_src[:1000]]\n",
        "train_trg_lengths = [len(tokenize_fr(s)) for s in train_trg[:1000]]\n",
        "\n",
        "print(f\"ƒê·ªô d√†i trung b√¨nh (EN): {np.mean(train_src_lengths):.1f} t·ª´\")\n",
        "print(f\"ƒê·ªô d√†i trung b√¨nh (FR): {np.mean(train_trg_lengths):.1f} t·ª´\")\n",
        "print(f\"ƒê·ªô d√†i max (EN): {np.max(train_src_lengths)} t·ª´\")\n",
        "print(f\"ƒê·ªô d√†i max (FR): {np.max(train_trg_lengths)} t·ª´\")\n",
        "\n",
        "# Teacher Forcing\n",
        "print(\"\\n2. TEACHER FORCING:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"‚úÖ ƒê√£ √°p d·ª•ng Scheduled Sampling:\")\n",
        "print(f\"   ‚Üí B·∫Øt ƒë·∫ßu: teacher_forcing_ratio = 1.0\")\n",
        "print(f\"   ‚Üí K·∫øt th√∫c: teacher_forcing_ratio = 0.5\")\n",
        "print(f\"   ‚Üí Gi·∫£m d·∫ßn theo epoch ƒë·ªÉ gi·∫£m exposure bias\")\n",
        "\n",
        "# Ki·ªÉm tra Overfitting\n",
        "print(\"\\n3. PH√íNG CH·ªêNG OVERFITTING:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"‚úÖ Dropout: {ENC_DROPOUT}\")\n",
        "print(f\"‚úÖ Early Stopping: patience = {PATIENCE}\")\n",
        "print(f\"‚úÖ Label Smoothing: 0.1\")\n",
        "print(f\"‚úÖ Weight Decay: 1e-5\")\n",
        "print(f\"‚úÖ Batch Size: {BATCH_SIZE}\")\n",
        "print(f\"üìä Best val loss: {best_val_loss:.3f}\")\n",
        "\n",
        "if len(train_losses) > 3:\n",
        "    last_train = train_losses[-1]\n",
        "    last_val = val_losses[-1]\n",
        "    gap = abs(last_val - last_train)\n",
        "\n",
        "    if gap < 1.0:\n",
        "        print(f\"‚úÖ M√¥ h√¨nh t·ªïng qu√°t h√≥a t·ªët (gap = {gap:.3f})\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è C√≥ d·∫•u hi·ªáu overfitting (gap = {gap:.3f})\")\n",
        "\n",
        "# Loss function\n",
        "print(\"\\n4. PH√ÇN T√çCH LOSS:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Loss function: CrossEntropyLoss\")\n",
        "print(f\"‚úÖ ignore_index=0 (lo·∫°i b·ªè padding token)\")\n",
        "print(f\"‚úÖ label_smoothing=0.1 (gi·∫£m overconfidence)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "6DeADUheS44e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C√¢u 6: Ph√¢n t√≠ch 5 v√≠ d·ª• d·ªãch + ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn\n",
        "\n"
      ],
      "metadata": {
        "id": "USY8PDg7S6Q9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1. Ph√¢n t√≠ch l·ªói ph·ªï bi·∫øn\n",
        "\n",
        "**5 c√¢u ph√¢n t√≠ch theo ƒë·ªô kh√≥:**\n",
        "- 2 c√¢u **D·ªÑ**: C√¢u ng·∫Øn, t·ª´ ph·ªï bi·∫øn (VD: \"A dog is running.\")\n",
        "- 2 c√¢u **TRUNG B√åNH**: C√¢u d√†i h∆°n, c·∫•u tr√∫c ph·ª©c t·∫°p (VD: \"A woman is reading a book in the park.\")\n",
        "- 1 c√¢u **KH√ì**: C√¢u d√†i, c√≥ th·ªÉ c√≥ t·ª´ hi·∫øm\n",
        "\n",
        "**L·ªói OOV (Out of Vocabulary):**\n",
        "- T·ª´ hi·∫øm ‚Üí `<UNK>`\n",
        "- ƒê·ªÅ xu·∫•t: BPE tokenization ho·∫∑c gi·∫£m freq_threshold\n",
        "\n",
        "**L·ªói c·∫•u tr√∫c c√¢u:**\n",
        "- C√¢u d√†i b·ªã qu√™n th√¥ng tin ƒë·∫ßu (do context vector c·ªë ƒë·ªãnh)\n",
        "- ƒê·ªÅ xu·∫•t: Attention Mechanism\n",
        "\n",
        "**L·ªói ng·ªØ ph√°p:**\n",
        "- Ch∆∞a h·ªçc ƒë·ªß patterns\n",
        "- ƒê·ªÅ xu·∫•t: Beam Search, more data"
      ],
      "metadata": {
        "id": "gtbTvKA1Dx7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# PH√ÇN T√çCH L·ªñI V√Ä ƒê·ªÄ XU·∫§T C·∫¢I TI·∫æN\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PH√ÇN T√çCH L·ªñI V√Ä ƒê·ªÄ XU·∫§T C·∫¢I TI·∫æN\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 6.1. Ph√¢n t√≠ch m·ªôt s·ªë c√¢u d·ªãch (D·ªÖ ‚Üí Trung b√¨nh ‚Üí Kh√≥)\n",
        "print(\"\\n6.1. PH√ÇN T√çCH C√ÇU D·ªäCH THEO ƒê·ªò KH√ì:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# 5 c√¢u ph√¢n t√≠ch: 2 d·ªÖ + 2 trung b√¨nh + 1 kh√≥\n",
        "analysis_samples = [\n",
        "    # 2 c√¢u D·ªÑ (c√¢u ng·∫Øn, t·ª´ ph·ªï bi·∫øn)\n",
        "    (\"A dog is running.\", \"D·ªÖ\"),\n",
        "    (\"The man is eating.\", \"D·ªÖ\"),\n",
        "    # 2 c√¢u TRUNG B√åNH (c√¢u d√†i h∆°n, c·∫•u tr√∫c ph·ª©c t·∫°p h∆°n)\n",
        "    (\"A woman is reading a book in the park.\", \"Trung b√¨nh\"),\n",
        "    (\"Two children are playing with a ball.\", \"Trung b√¨nh\"),\n",
        "    # 1 c√¢u KH√ì (c√¢u d√†i, c√≥ th·ªÉ c√≥ t·ª´ hi·∫øm)\n",
        "    (\"The professional photographer is capturing beautiful moments during sunset.\", \"Kh√≥\"),\n",
        "]\n",
        "\n",
        "for idx, (en_sentence, difficulty) in enumerate(analysis_samples, 1):\n",
        "    pred = translate_sentence(en_sentence, spacy_en, vocab_en, vocab_fr, model, device)\n",
        "    pred_text = \" \".join(pred)\n",
        "\n",
        "    print(f\"\\nüìå C√¢u {idx} [{difficulty}]\")\n",
        "    print(f\"üá¨üáß EN: {en_sentence}\")\n",
        "    print(f\"üá´üá∑ FR: {pred_text}\")\n",
        "\n",
        "    # ƒê·∫øm s·ªë t·ª´ <UNK>\n",
        "    unk_count = sum(1 for token in tokenize_en(en_sentence) if token not in vocab_en.stoi)\n",
        "    if unk_count > 0:\n",
        "        print(f\"   ‚ö†Ô∏è Ph√°t hi·ªán {unk_count} t·ª´ OOV (kh√¥ng c√≥ trong t·ª´ ƒëi·ªÉn)\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ T·∫•t c·∫£ t·ª´ ƒë·ªÅu c√≥ trong t·ª´ ƒëi·ªÉn\")\n",
        "\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(\"üìä NH·∫¨N X√âT:\")\n",
        "print(\"   ‚Ä¢ C√¢u D·ªÑ: Th∆∞·ªùng d·ªãch ch√≠nh x√°c v√¨ t·ª´ ph·ªï bi·∫øn\")\n",
        "print(\"   ‚Ä¢ C√¢u TRUNG B√åNH: C√≥ th·ªÉ sai ng·ªØ ph√°p nh·ªè\")\n",
        "print(\"   ‚Ä¢ C√¢u KH√ì: D·ªÖ c√≥ <UNK> v√† m·∫•t th√¥ng tin ng·ªØ c·∫£nh\")\n",
        "\n",
        "# 6.2. Th·ªëng k√™ t·ª´ v·ª±ng\n",
        "print(\"\\n\\n6.2. TH·ªêNG K√ä T·ª™ V·ª∞NG:\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"üìñ K√≠ch th∆∞·ªõc vocab EN: {len(vocab_en):,} t·ª´\")\n",
        "print(f\"üìñ K√≠ch th∆∞·ªõc vocab FR: {len(vocab_fr):,} t·ª´\")\n",
        "print(f\"üîß Frequency threshold: {vocab_en.freq_threshold}\")\n",
        "print(f\"\\nüí° N·∫øu vocab nh·ªè ‚Üí nhi·ªÅu <UNK> ‚Üí d·ªãch k√©m ch√≠nh x√°c\")\n",
        "print(f\"   Gi·∫£i ph√°p: Gi·∫£m freq_threshold ho·∫∑c d√πng BPE tokenization\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)"
      ],
      "metadata": {
        "id": "z8yItz6HS8bn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.2. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn\n",
        "\n",
        "**C√°c k·ªπ thu·∫≠t c·∫£i ti·∫øn ki·∫øn tr√∫c:**\n",
        "\n",
        "1. **Attention Mechanism (C∆° ch·∫ø T·∫≠p trung)**\n",
        "   - Gi√∫p m√¥ h√¨nh \"nh√¨n\" v√†o t·ª´ng ph·∫ßn quan tr·ªçng c·ªßa c√¢u ngu·ªìn khi decode\n",
        "   - Thay v√¨ d√πng 1 context vector c·ªë ƒë·ªãnh, decoder c√≥ th·ªÉ truy c·∫≠p to√†n b·ªô hidden states c·ªßa encoder\n",
        "   \n",
        "2. **Beam Search (T√¨m ki·∫øm Ch√πm)**\n",
        "   - T√¨m ki·∫øm nhi·ªÅu kh·∫£ nƒÉng (k candidates) thay v√¨ ch·ªâ 1 (greedy)\n",
        "   - Gi√∫p t√¨m ƒë∆∞·ª£c b·∫£n d·ªãch t·ªïng th·ªÉ t·ªët h∆°n b·∫±ng c√°ch xem x√©t nhi·ªÅu ƒë∆∞·ªùng ƒëi\n",
        "   \n",
        "3. **Subword Tokenization - BPE (Byte Pair Encoding)**\n",
        "   - Chia t·ª´ th√†nh c√°c ƒë∆°n v·ªã nh·ªè h∆°n (v√≠ d·ª•: \"playing\" ‚Üí \"play\" + \"ing\")\n",
        "   - X·ª≠ l√Ω t·ª´ hi·∫øm v√† t·ª´ ngo√†i t·ª´ ƒëi·ªÉn (OOV) hi·ªáu qu·∫£ h∆°n\n",
        "   \n",
        "4. **Transformer**\n",
        "   - Ki·∫øn tr√∫c hi·ªán ƒë·∫°i thay th·∫ø LSTM\n",
        "   - S·ª≠ d·ª•ng self-attention, x·ª≠ l√Ω song song, nhanh v√† ch√≠nh x√°c h∆°n\n",
        "\n",
        "**K·ªπ thu·∫≠t c·∫£i ti·∫øn hu·∫•n luy·ªán:**\n",
        "1. ReduceLROnPlateau - Gi·∫£m learning rate khi loss kh√¥ng gi·∫£m\n",
        "2. Scheduled Sampling - Gi·∫£m d·∫ßn teacher forcing theo th·ªùi gian\n",
        "3. Data Augmentation - TƒÉng d·ªØ li·ªáu b·∫±ng back-translation"
      ],
      "metadata": {
        "id": "M3jUbVgFS-fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6.2. ƒê·ªÅ xu·∫•t c·∫£i ti·∫øn\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ƒê·ªÄ XU·∫§T C·∫¢I TI·∫æN\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n C√ÅC K·ª∏ THU·∫¨T C·∫¢I TI·∫æN KI·∫æN TR√öC:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "architecture_improvements = [\n",
        "    (\"Attention Mechanism\",\n",
        "     \"Gi√∫p decoder 'ch√∫ √Ω' v√†o t·ª´ng ph·∫ßn c√¢u ngu·ªìn thay v√¨ d√πng 1 context vector c·ªë ƒë·ªãnh\",\n",
        "     \"TƒÉng BLEU 5-10%, ƒë·∫∑c bi·ªát t·ªët v·ªõi c√¢u d√†i\"),\n",
        "\n",
        "    (\"Beam Search\",\n",
        "     \"Gi·ªØ l·∫°i k kh·∫£ nƒÉng t·ªët nh·∫•t m·ªói b∆∞·ªõc thay v√¨ ch·ªâ ch·ªçn 1 t·ª´ (greedy decoding)\",\n",
        "     \"C·∫£i thi·ªán ch·∫•t l∆∞·ª£ng d·ªãch 2-5% BLEU\"),\n",
        "\n",
        "    (\"Subword (BPE)\",\n",
        "     \"Chia t·ª´ th√†nh ƒë∆°n v·ªã nh·ªè h∆°n, x·ª≠ l√Ω t·ª´ hi·∫øm t·ªët h∆°n (VD: 'playing' = 'play'+'ing')\",\n",
        "     \"Gi·∫£m t·ª∑ l·ªá <UNK> t·ª´ 5-10% xu·ªëng g·∫ßn 0%\"),\n",
        "\n",
        "    (\"Transformer\",\n",
        "     \"Thay LSTM b·∫±ng self-attention, x·ª≠ l√Ω song song, nhanh v√† ch√≠nh x√°c h∆°n\",\n",
        "     \"SOTA cho d·ªãch m√°y, BLEU tƒÉng 10-15%\"),\n",
        "]\n",
        "\n",
        "for i, (method, desc, impact) in enumerate(architecture_improvements, 1):\n",
        "    print(f\"\\n{i}. {method}\")\n",
        "    print(f\"   M√¥ t·∫£: {desc}\")\n",
        "    print(f\"   T√°c ƒë·ªông: {impact}\")\n",
        "\n",
        "print(\"\\n\\n C√ÅC K·ª∏ THU·∫¨T C·∫¢I TI·∫æN HU·∫§N LUY·ªÜN:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "training_improvements = [\n",
        "    (\"ReduceLROnPlateau\", \"T·ª± ƒë·ªông gi·∫£m learning rate khi val_loss kh√¥ng gi·∫£m\"),\n",
        "    (\"Scheduled Sampling\", \"Gi·∫£m d·∫ßn teacher forcing t·ª´ 1.0 ‚Üí 0.5 theo epoch\"),\n",
        "    (\"Data Augmentation\", \"TƒÉng d·ªØ li·ªáu b·∫±ng back-translation (d·ªãch FR‚ÜíEN‚ÜíFR)\"),\n",
        "]\n",
        "\n",
        "for i, (method, desc) in enumerate(training_improvements, 1):\n",
        "    print(f\"{i}. {method:25s} - {desc}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"HO√ÄN T·∫§T PH√ÇN T√çCH!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "id": "EyWEcc2BS_HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hiLPNL_RL3Di"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}